{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#         Machine Learning in Python & Hands-on Training\n",
    "\n",
    "<a style=\"display: inline-block;\" href=\"https://mybinder.org/v2/gh/RadiantMLHub/ml4eo-bootcamp-2021/main?filepath=Lecture%206%2Fmachine_learning_python.ipynb\"><img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Launch in Binder\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to perform drone imagery classification on the training dataset for crop types in Rwanda ([Dataset Description](http://registry.mlhub.earth/10.34911/rdnt.r4p1fr/)).\n",
    "\n",
    "The dataset contains 2606 images with 6 crop type labels classified as:\n",
    "\n",
    " \"banana\",\"maize\",\"other\",\"forest\",\"legumes\",\"structure\"\n",
    "                    \n",
    "The goal of this tutorial is to:\n",
    "1. Obtain the data from Radiant MLHub's Python client\n",
    "2. Extract this data and prepare it to build a ML model\n",
    "3. Training and evaluating the model\n",
    "4. Transfer learning using the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the Data\n",
    "\n",
    "The documentation of Radiant MLHub client is available at: https://radiant-mlhub.readthedocs.io/en/latest/getting_started.html#installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need an API key to implement the library.\n",
    "To obtain that:\n",
    "1. Sign up on MLHub and generate an API key (steps are available in the documentation referenced earlier)\n",
    "2. Use the following on the Anaconda prompt or command line: `mlhub configure`\n",
    "\n",
    "The command prompt diplays the following message:\n",
    "\n",
    "'API Key: Enter your API key here...'\n",
    "\n",
    "3. Copy and paste the API key generated earlier\n",
    "\n",
    "Wrote profile to C:\\Users\\Maximus\\.mlhub\\profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idiv_asia_crop_type             : A crop type dataset for consistent land cover classification in Central Asia\n",
      "bigearthnet_v1                  : BigEarthNet\n",
      "microsoft_chesapeake            : Chesapeake Land Cover\n",
      "ref_african_crops_kenya_02      : CV4A Kenya Crop Type Competition\n",
      "ref_african_crops_uganda_01     : Dalberg Data Insights Crop Type Uganda\n",
      "rti_rwanda_crop_type            : Drone Imagery Classification Training Dataset for Crop Types in Rwanda\n",
      "ref_african_crops_tanzania_01   : Great African Food Company Crop Type Tanzania\n",
      "landcovernet_v1                 : LandCoverNet\n",
      "open_cities_ai_challenge        : Open Cities AI Challenge\n",
      "ref_african_crops_kenya_01      : PlantVillage Crop Type Kenya\n",
      "su_african_crops_ghana          : Semantic Segmentation of Crop Type in Ghana\n",
      "su_african_crops_south_sudan    : Semantic Segmentation of Crop Type in South Sudan\n",
      "sen12floods                     : SEN12-FLOOD\n",
      "ts_cashew_benin                 : Smallholder Cashew Plantations in Benin\n",
      "spacenet1                       : Spacenet 1\n",
      "spacenet2                       : Spacenet 2\n",
      "spacenet3                       : Spacenet 3\n",
      "spacenet4                       : Spacenet 4\n",
      "spacenet5                       : Spacenet 5\n",
      "spacenet6                       : Spacenet 6\n",
      "spacenet7                       : Spacenet 7\n",
      "nasa_tropical_storm_competition : Tropical Cyclone Wind Estimation Competition\n",
      "su_sar_moisture_content_main    : Western USA Live Fuel Moisture\n"
     ]
    }
   ],
   "source": [
    "#let's view the datasets contained in the data registry\n",
    "from radiant_mlhub import Dataset, Collection #Collection module will be used for viewing collection ids\n",
    "for dataset in Dataset.list():\n",
    "    print(f'{dataset.id:<32}: {dataset.title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use rti rwanda crop type here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Collection id=rti_rwanda_crop_type_labels>, <Collection id=rti_rwanda_crop_type_source>, <Collection id=rti_rwanda_crop_type_raw>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.fetch('rti_rwanda_crop_type')\n",
    "dataset.collections #viewing the collection ids under the crop type dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of:\n",
    "\n",
    "1. Collection id = `rti_rwanda_crop_type_labels`\n",
    "\n",
    "2. Collection id = `rti_rwanda_crop_type_source`\n",
    "\n",
    "3. Collection id = `rti_rwanda_crop_type_raw`\n",
    "\n",
    "In this exercise, we will use only the first two collections.\n",
    "\n",
    "`Source` collection contains input images for ML model training, and\n",
    "\n",
    "`Labels` collection contains the tags/classes/labels for the images.\n",
    "\n",
    "We will download both of them by first 'fetching' their contents, then 'downloading' them to the local system. The Collection module provides the method to download the data.\n",
    "\n",
    "For this tutorial, we will use the local 'Downloads' folder for data storage and access.\n",
    "**Note** if you are running this in Google Colab or Binder you need to create a Downloads folder, or change the path in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "downloads_path = str(Path.home() / \"Downloads\") #making a relative path wrt the Downloads folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dacc3b4f691841ffba05fa7f326e5cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/0.8 [00:00<?, ?M/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/hamed/Downloads/Data/rti_rwanda_crop_type_labels.tar.gz')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = Collection.fetch('rti_rwanda_crop_type_labels')#downloading the labels\n",
    "collection.download(downloads_path+'/Data')  # Will raise exception if the file already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0857e8328610437299f228bf997031b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104.5 [00:00<?, ?M/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/hamed/Downloads/Data/rti_rwanda_crop_type_source.tar.gz')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = Collection.fetch('rti_rwanda_crop_type_source') #downloading the source images\n",
    "collection.download(downloads_path+'/Data')   # Will raise exception if the file already exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the data\n",
    "\n",
    "The data is then extracted into an \"Extracted\" folder.\n",
    "\n",
    "What we'll do next is to match each label to the respective image it points to.\n",
    "\n",
    "To do this, we will create a list that will contain the addresses of each of the labels called \"source\".\n",
    "There will also be a list containing the addresses of each of the source images called \"destination\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.unpack_archive(downloads_path+'/Data/rti_rwanda_crop_type_source.tar.gz', downloads_path+'/Extracted')\n",
    "shutil.unpack_archive(downloads_path+'/Data/rti_rwanda_crop_type_labels.tar.gz', downloads_path+'/Extracted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documentation\n",
    "Note that more details about the dataset is available in the documentation file located at `_common/documentation.pdf` in the extracted labels folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #to access the files on the local system\n",
    "labels_path = downloads_path+ \"/Extracted/rti_rwanda_crop_type_labels\" #label files\n",
    "sources_path = downloads_path + \"/Extracted/rti_rwanda_crop_type_source\" #source image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the path to all image source files\n",
    "source_images = []\n",
    "for root, dirs, files in os.walk(sources_path):\n",
    "    for file in files:\n",
    "        if(file.endswith(\".png\")):\n",
    "            source_images.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/hamed/Downloads/Extracted/rti_rwanda_cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/hamed/Downloads/Extracted/rti_rwanda_cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/hamed/Downloads/Extracted/rti_rwanda_cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/hamed/Downloads/Extracted/rti_rwanda_cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/hamed/Downloads/Extracted/rti_rwanda_cr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         source_file\n",
       "0  /Users/hamed/Downloads/Extracted/rti_rwanda_cr...\n",
       "1  /Users/hamed/Downloads/Extracted/rti_rwanda_cr...\n",
       "2  /Users/hamed/Downloads/Extracted/rti_rwanda_cr...\n",
       "3  /Users/hamed/Downloads/Extracted/rti_rwanda_cr...\n",
       "4  /Users/hamed/Downloads/Extracted/rti_rwanda_cr..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset_df = pd.DataFrame(data = source_images,columns=[\"source_file\"])\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hamed/Downloads/Extracted/rti_rwanda_crop_type_source/rti_rwanda_crop_type_source_2049'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample path to a source file\n",
    "dataset_df[\"source_file\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "def get_label(source_file, labels_path):\n",
    "    image_id = source_file.split('_')[-1]\n",
    "    label_file = labels_path + '/rti_rwanda_crop_type_labels_' + str(image_id) + '/labels.json'\n",
    "    #load label\n",
    "    with open(label_file) as file:\n",
    "        label = json.load(file)[\"label\"]\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label column to pandas\n",
    "dataset_df[\"label\"] = dataset_df[\"source_file\"].apply(get_label, args=(labels_path,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_file</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/hamed/Downloads/Extracted/rti_rwanda_cr...</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/hamed/Downloads/Extracted/rti_rwanda_cr...</td>\n",
       "      <td>banana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/hamed/Downloads/Extracted/rti_rwanda_cr...</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/hamed/Downloads/Extracted/rti_rwanda_cr...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/hamed/Downloads/Extracted/rti_rwanda_cr...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         source_file   label\n",
       "0  /Users/hamed/Downloads/Extracted/rti_rwanda_cr...  forest\n",
       "1  /Users/hamed/Downloads/Extracted/rti_rwanda_cr...  banana\n",
       "2  /Users/hamed/Downloads/Extracted/rti_rwanda_cr...  forest\n",
       "3  /Users/hamed/Downloads/Extracted/rti_rwanda_cr...   other\n",
       "4  /Users/hamed/Downloads/Extracted/rti_rwanda_cr...   other"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target folder for each each label\n",
    "# define a list of keywords based on the label classes ()\n",
    "keys = (\n",
    "    \"banana\",\n",
    "    \"maize\",\n",
    "    \"other\",\n",
    "    \"forest\",\n",
    "    \"legumes\",\n",
    "    \"structure\"\n",
    ")\n",
    "if not os.path.isdir(downloads_path + '/Images/'):\n",
    "    os.makedirs(downloads_path + '/Images/')\n",
    "for key in keys:\n",
    "    if not os.path.isdir(downloads_path + '/Images/' + key):\n",
    "        os.mkdir(downloads_path + '/Images/' + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying source imagery to corresponding label folders\n",
    "\n",
    "for key in keys:\n",
    "    key_df = dataset_df[dataset_df[\"label\"]==key]\n",
    "    key_df = key_df.reset_index()\n",
    "    for row in key_df.iterrows():\n",
    "        shutil.copy(row[1][\"source_file\"] + '/Image.png', downloads_path + '/Images/' + key + '/' + str(row[1]['index']) + '.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Machine Learning Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to install tensorflow and tensorflow_hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also import the needed libraries. Notes on these libraries are commented in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals #for newer features of python\n",
    "import matplotlib.pylab as plt #graphical plots\n",
    "import tensorflow as tf #deep learning\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np #numerical procedures\n",
    "import pandas as pd #data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.version.VERSION #tensorflow version used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.precision\", 6) #setting display precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root=downloads_path + '\\\\Images' #access to the data on local system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For building the model, we will also need keras for the convolutional layers, hence keras must be installed and thereafter, imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing everything related to keras and building the model\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.convolutional import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "\n",
    "#after building the model, we will make use of these for model evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import itertools #for iterations\n",
    "import time #for saving the model with respect to the system time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, we're going to use a multi-class model. We will use the following classes:\n",
    "- banana\n",
    "- forest\n",
    "- legumes\n",
    "- maize\n",
    "- other\n",
    "- structure\n",
    "\n",
    "The other images are excluded from the \"Images\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedClasses = (os.listdir(data_root)) #displaying the path and folders available\n",
    "print (selectedClasses) #classes from the Images folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have < 2606 images across the folders. These may not be enough to train the model appropriately.\n",
    "Hence, to make more images available for training, we will apply *data augmentation.*\n",
    "\n",
    "Data augmentation, in data science are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model.\n",
    "\n",
    "Data augmentation is applied below with comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_kwargs = dict(rescale=1./255, validation_split=.20) #rescale is normalisation for numerical stability and convergence\n",
    "train_datagen = ImageDataGenerator(\n",
    "    **datagen_kwargs,\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        \n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data augmentation, i choose to :\n",
    "\n",
    "Randomly rotate some training images by 10 degrees\n",
    "\n",
    "Randomly Zoom by 10% some training images\n",
    "\n",
    "Randomly shift images horizontally by 10% of the width\n",
    "\n",
    "Randomly shift images vertically by 10% of the height\n",
    "\n",
    "Documentation is available at: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "\n",
    "Once our model is ready, we fit the training, validation and test dataset below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_DIR = str(data_root) #data directory for training\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAINING_DATA_DIR,\n",
    "    target_size=(224, 224), #image size\n",
    "    shuffle = True, #randomize the data for training\n",
    "    subset='training') # set as training data\n",
    "\n",
    "#validation data is just a subset of training data, hence, we use train_datagen for the validation generator too\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    TRAINING_DATA_DIR, # same directory as training data\n",
    "    target_size=(224, 224),\n",
    "    shuffle = True,\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not have abundant data here, we will create the test data based on the available data.\n",
    "\n",
    "We selected the test data to be 30% of the data. 15% or 20% are more common splits.\n",
    "\n",
    "The test data will thus share some data with the validation data but not all, since the validation data is randomised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_kwargs = dict(rescale=1./255,validation_split=.3) #for  the test data. 30% of training data\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "TRAINING_DATA_DIR,\n",
    "subset=\"training\",\n",
    "shuffle=False,\n",
    "target_size=(224,224)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to plot some samples of the images in the dataset. However, we will need to instantiate a duplicate train_generator, but without the normalisation.\n",
    "\n",
    "This is because, with normaliation, the ouput sample images will just be a list of dark images without any identity in this case.\n",
    "\n",
    "The duplicate is initialised as 'train_generator_plot'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen_plot = ImageDataGenerator()\n",
    "train_generator_plot = train_datagen_plot.flow_from_directory(\n",
    "    TRAINING_DATA_DIR,\n",
    "    target_size=(224, 224), #image size\n",
    "    shuffle = True, #randomize the data for training\n",
    "    subset='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"In train_generator \")\n",
    "for cls in range(len (train_generator.class_indices)):\n",
    "    print(selectedClasses[cls],\":\\t\",list(train_generator.classes).count(cls))\n",
    "    #displaying item number for train_generator\n",
    "print (\"\") \n",
    "\n",
    "print (\"In validation_generator \")\n",
    "for cls in range(len (validation_generator.class_indices)):\n",
    "    print(selectedClasses[cls],\":\\t\",list(validation_generator.classes).count(cls))\n",
    "    #displaying item number for train_generator\n",
    "print (\"\") \n",
    "\n",
    "print (\"In test_generator \")\n",
    "for cls in range(len (test_generator.class_indices)):\n",
    "    print(selectedClasses[cls],\":\\t\",list(test_generator.classes).count(cls))\n",
    "    #displaying item number for train_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will randomly plot images just to quickly inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots images with labels within jupyter notebook\n",
    "def plots(ims, figsize = (10,10), rows=4, interp=False, titles=None, maxNum = 9):\n",
    "    if type(ims[0] is np.ndarray):\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if(ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0,2,3,1))\n",
    "           \n",
    "    f = plt.figure(figsize=figsize)\n",
    "    #cols = len(ims) //rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
    "    cols = maxNum // rows if maxNum % 2 == 0 else maxNum//rows + 1\n",
    "    #for i in range(len(ims)):\n",
    "    for i in range(maxNum):\n",
    "        sp = f.add_subplot(rows, cols, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=20)\n",
    "        plt.imshow(ims[i], interpolation = None if interp else 'none')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_plot.reset()\n",
    "imgs, labels = train_generator_plot.next() #assign the images you want to plot in here\n",
    "\n",
    "#print(labels)\n",
    "\n",
    "labelNames=[]\n",
    "labelIndices=[np.where(r==1)[0][0] for r in labels]\n",
    "#print(labelIndices)\n",
    "\n",
    "for ind in labelIndices:\n",
    "    for labelName,labelIndex in train_generator.class_indices.items():\n",
    "        if labelIndex == ind:\n",
    "            #print (labelName)\n",
    "            labelNames.append(labelName)\n",
    "\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots(imgs, rows=4, titles = labelNames, maxNum=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin building and training the model soon, but first, we need to initialize a filepath where the model gets repeatedly updated as the validation accuracy increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatic rename with epoch number and val accuracy:\n",
    "#filepath=\"checkpoints/weights-improvement-epeoch-{epoch:02d}-val_acc-{val_acc:.2f}.hdf5\"\n",
    "\n",
    "\n",
    " \n",
    "modelName= \"Model\"\n",
    "#save the best weights over the same file with the model name\n",
    "\n",
    "#filepath=\"checkpoints/\"+modelName+\"_bestweights.hdf5\"\n",
    "filepath=modelName+\"_bestweights.hdf5\" #it is in the same folder as where the notebook is\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') #max validation accuracy\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the model next. We have the following model choices:\n",
    "\n",
    "* **The simple Convolution Neural Network (CNN)** is especially useful for binary image classification and simple multiclass cases.\n",
    "However, some drawbacks include classification of images with different positions. It also requires a large dataset to process and train the neural network, which we do not have the abundance of in this case. It could also be slow at times.\n",
    "\n",
    "\n",
    "* **RESNET**, another option makes it possible to train up to hundreds or even thousands of layers and still achieves compelling performance. It however, has increased complexity of architecture and could be time-consuming, especially without a GPU in this case.\n",
    "\n",
    "\n",
    "* **VGG** is a very good architecture for benchmarking on a particular task with high accuracy.\n",
    "\n",
    "\n",
    "* The model we're using is **Tensorflow's Mobilenet_V2 model** based on ImageNet.\n",
    "It is a very handy deep learning model because it is a small, low-latency convolutional neural network.\n",
    "It has about the same accuracy as VGG but MobileNet is about 32 times smaller than VGG (VGG16), yet has the same accuracy, it must be more efficient at capturing knowledge than VGG is. It's indeed a known fact that VGG has way more connections than it needs in order to do its job.\n",
    "It's also quite fast and accurate, while having a relatively portable size (14mb), which is the smallest of all Deep Convolutional Neural Network Architectures.\n",
    "\n",
    "\n",
    "* **InceptionV3 and Xception** are also very efficient models. They have better accuracies than Mobilenet but are larger, more complex and would require GPU to train models without causing the system to hang/fail. \n",
    "\n",
    "Documentation for MobileNetV2 is available at: https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the MobileNet model from TensorFlow\n",
    "model = tf.keras.Sequential([\n",
    "hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\",\n",
    "output_shape=[1280],\n",
    "trainable=False),\n",
    "tf.keras.layers.Dropout(0.4), #prevent a model from overfitting\n",
    "tf.keras.layers.Dense(train_generator.num_classes, activation='softmax') #works on the input and returns the output\n",
    "#Softmax is often used as the activation for the last layer of a classification network\n",
    "#because the result could be interpreted as a probability distribution\n",
    "])\n",
    "model.build([None, 224, 224, 3]) #based on the input shape we specified earlier\n",
    "model.summary()\n",
    "model.compile(\n",
    "optimizer=tf.keras.optimizers.Adam(),\n",
    "loss='categorical_crossentropy', #for multi-class\n",
    "metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize=32 #default practice\n",
    "stepsPerEpoch= (train_generator.samples+ (batchSize-1)) // batchSize\n",
    "print(\"stepsPerEpoch: \", stepsPerEpoch) #formula\n",
    "\n",
    "validationSteps=(validation_generator.samples+ (batchSize-1)) // batchSize\n",
    "print(\"validationSteps: \", validationSteps) #formula\n",
    "\n",
    "\n",
    "#validationSteps=(test_generator.samples+ (batchSize-1)) // batchSize\n",
    "#print(\"validationSteps: \", validationSteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch represents each instance the model trains. stepsPerEpoch represents the number of steps the model takes to evaluate itself for each epoch with respect to the training data.\n",
    "\n",
    "validationSteps is the stepsPerEpoch, but for the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit_generator(\n",
    "    train_generator, \n",
    "    validation_data = validation_generator,\n",
    "    epochs = 30, #just a significant number\n",
    "    steps_per_epoch = stepsPerEpoch,\n",
    "    validation_steps= validationSteps,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a peak validation accuracy of 80.5%, which is good enough in this case.\n",
    "\n",
    "We will then plot the accuracy and loss graphs for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_details(train):  \n",
    "  plt.figure(figsize=[10,7]) #setting figure size\n",
    "  plt.plot(train.history['acc'],'r',linewidth=3.0) #first line for training accuracy\n",
    "  plt.plot(train.history['val_acc'],'b',linewidth=3.0) #second line for validation accuracy\n",
    "  plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=12)\n",
    "  plt.xlabel('Epochs ',fontsize=16)\n",
    "  plt.ylabel('Accuracy',fontsize=16)\n",
    "  plt.title('Accuracy Curves',fontsize=16)\n",
    "\n",
    "  # Loss Curves\n",
    "  plt.figure(figsize=[10,7]) \n",
    "  plt.plot(train.history['loss'],'r',linewidth=3.0) #training loss\n",
    "  plt.plot(train.history['val_loss'],'b',linewidth=3.0) #validation loss\n",
    "  plt.legend(['Training loss', 'Validation Loss'],fontsize=12)\n",
    "  plt.xlabel('Epochs ',fontsize=16)\n",
    "  plt.ylabel('Loss',fontsize=16)\n",
    "  plt.title('Loss Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_details(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "The process will be thus:\n",
    "* Save the model and the weights for the neural-network.\n",
    "* Then, the model and its best weights are uploaded. That is, the model is treated as a new model being applied to a new process. This is the process of **transfer learning**.\n",
    "* The model is setup as it attempts to predict the fresh test dataset, which has been independent of the training and parameter tuning done earlier.\n",
    "* The model is evaluated against the test dataset. Its accuracy for the test-set will be checked. The precision for every class will also be checked as well as the confusion matrix graph. These will help us understand the application of the model from a practical perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model and Last Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(timestr+\"_\"+modelName+\"_MODEL_3\"+\".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(timestr+\"_\"+modelName+\"_3_LAST_WEIGHTS_\"+\".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the Model and Best Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('20210430_112038_Model_MODEL_3.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json, custom_objects={'KerasLayer': hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights into new model\n",
    "model.load_weights(\"Model_bestweights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Model to Work on Test Set of Images\n",
    "\n",
    "We initialized the 'predictions' variable. This will store the model's predictions on the test set.\n",
    "\n",
    "It took 50 seconds to make 779 predictions, showing the time-effectiveness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "testStep = (test_generator.samples + (batchSize-1)) // batchSize\n",
    "print(\"testStep: \", testStep)\n",
    "predictions = model.predict_generator(test_generator, steps = testStep ,  verbose = 1)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices=np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stored a 'y_pred' variable. This will contain the predicted results in an array format for numerical data calculations.\n",
    "\n",
    "It is shown as a numpy array type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict_classes(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, 'y_true' represents the actual class of the image to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=np.array(test_generator.classes)\n",
    "y_true = y_true.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (test_generator.class_indices) #displaying the labels/classes of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dict((v,k) for k,v in labels.items()) #converting to a dictionary, which is more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualLables= [labels[k] for k in test_generator.classes] #actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedLables= [labels[k] for k in predicted_class_indices] #predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model's Performance on the Test Data\n",
    "\n",
    "The first step taken is evaluating the accuracy.\n",
    "Accuracy represents the rate at which the model classifies an image correctly.\n",
    "\n",
    "#### * In this case, accuracy represents the model's efficiency in correctly classifying a crop image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(actualLables, predictedLables) #evaluating the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACCURACY: 71%\n",
    "\n",
    "A decent accuracy of 70.86% ≈ 71% was obtained. This indicates a good performance of the model, given available data.\n",
    "\n",
    "However, evaluating the accuracy is not sufficient. We need to determine other parameters such as precision and recall.\n",
    "\n",
    "### The precision and recall metrics\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "Several metrics can be derived from a confusion matrix. (See the Wikipedia article.) In particular, they tend to be based on the special case of a confusion matrix, where we assign one class to be the \"positive\" class that is important to us. This is sometimes called a table of confusion. In such a table, we speak of true positives, false positives, false negatives, and true negatives.\n",
    "\n",
    "The precision and recall metrics are probably the most common metrics derived from such a table.\n",
    "\n",
    "**Precision (P) = TP / (TP + FP)**\n",
    "\n",
    "**Recall (R) = TP / (TP + FN)**\n",
    "\n",
    "When a search engine returns 30 pages, only 20 of which are relevant, while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3, which tells us how valid the results are, while its recall is 20/60 = 1/3, which tells us how complete the results are.\n",
    "\n",
    "For example, What's the precision and recall of 'banana' in this case?\n",
    "\n",
    "The utility function classification_report prints the precision and recall values for all the categories. (The F1 score combines the precision and recall values into a single value.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "report = metrics.classification_report(y_true, y_pred, target_names=selectedClasses)\n",
    "print(report)   #displaying the results\n",
    "#f1-score is a measure of the test's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model does best in evaluating classes 'banana', 'maize' and 'structure', but not very well with 'legumes' and 'other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score( actualLables, predictedLables,average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall score: 0.71\n",
    "\n",
    "This is a decent score. A perfect recall score is 1, hence our recall score and the model are great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score( actualLables, predictedLables,average='weighted') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision score: 0.75\n",
    "\n",
    "This is a very impressive score. A perfect precision score is 1, hence our precision score is great and the model is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the confusion matrix\n",
    "\n",
    "The confusion matrix gives a very good visual of the accuracy, precision and recall.\n",
    "\n",
    "It also helps us see the details of each crop-type classification, which class was most incorrectly evaluated for each item, etc.\n",
    "\n",
    "In summary, it gives a good intuition of the data and its classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seaborn module will be used to visualize the confusion matrix\n",
    "import seaborn as sn \n",
    "cm = metrics.confusion_matrix(y_true, y_pred) #initialising the confusion matrix 'cm'\n",
    "\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize = (20,20)) #setting an appropriate and legible diagram size\n",
    "sn.set(font_scale=2) #increasing font for legibility\n",
    "# Labels, title and ticks\n",
    "ax=sn.heatmap(cmn, xticklabels = selectedClasses, yticklabels = selectedClasses, annot=True)\n",
    "ax.set(xlabel='Predicted labels', ylabel='True labels', title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the highlighted results for the actual class vs the predicted class is weighted in terms of its recall.\n",
    "\n",
    "This is because it combines the true positive and the total false negatives on the horizontal axis with the total being 1 (100%).\n",
    "\n",
    "The precision rather combines the true positive and false positive for each class on the vertical axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim and objectives of the tutorials have been successfully completed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
