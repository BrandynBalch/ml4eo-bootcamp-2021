{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tropical Cyclone Wind Estimation from Satellite Imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jclFfXEq3Doh"
   },
   "source": [
    "By: Ali Ahmadalipour, Principal Data Scientist at KatRisk\n",
    "\n",
    "To check out a summary of the methodology and results, please visit https://www.linkedin.com/pulse/deep-learning-computer-vision-satellite-imagery-ali-ahmadalipour/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efPlyRRWQ0m-"
   },
   "source": [
    "# 1. Import data and unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FxemlEc4mDX"
   },
   "source": [
    "This section will download the data from Radiant MLHub. \n",
    "\n",
    "But for this presentation, we will skip running it, since I have alreay downloaded and preprocessed the data to save time during the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeSR9ITuR_bl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhEYctoi1rko"
   },
   "source": [
    "### Download data using Radiant MLHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sOfRJ9Qr1_Ij"
   },
   "outputs": [],
   "source": [
    "# pip install radiant_mlhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RgH61oWaF-o"
   },
   "outputs": [],
   "source": [
    "from radiant_mlhub import Dataset, Collection, client\n",
    "#os.environ['MLHUB_API_KEY'] = 'YOUR_MLHub_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEuVk8r74H_j"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.fetch('nasa_tropical_storm_competition')\n",
    "download_dir = Path('./').resolve()\n",
    "archive_paths = dataset.download(output_dir=download_dir)\n",
    "#print('Download completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2yYKmo_44Vv"
   },
   "source": [
    "### Check the number of files to ensure correct extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D_-pSQAfSCxe"
   },
   "outputs": [],
   "source": [
    "## Count the number of files to make sure that all files are correctly loaded and extracted:\n",
    "#directory ='.'\n",
    "#path, dirs, files = next(os.walk(directory+'/train'))\n",
    "#file_count = len(files)\n",
    "#file_count #70257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7CnSjksSDGF"
   },
   "source": [
    "# 2. Preprocessing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xsGPyUdSSKQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the pre-processed data from this shared Google Drive folder:\n",
    "\n",
    "https://drive.google.com/drive/folders/1jTw5UoLewMsD0SckoyTcfbN7UNWXl9EN?usp=sharing\n",
    "\n",
    "To load the data from the shared Google drive, open the shared drive in your browser,\n",
    "right click on the folder, and select \"Add shortcut to Drive\". Now you should see it on your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount your Google drive to this notebook\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRg6PFZNSSY-"
   },
   "outputs": [],
   "source": [
    "directory ='/content/drive/MyDrive/Lecture_10/' \n",
    "train_metadata  = pd.read_csv(f'{directory}/training_set_features.csv')\n",
    "train_labels = pd.read_csv(f'{directory}/training_set_labels.csv')\n",
    "#directory = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "NI09U4ZfU1xV",
    "outputId": "fdffb4e9-6889-41ac-bee8-06bcd8012a30"
   },
   "outputs": [],
   "source": [
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "O1FDRGHtU4I0",
    "outputId": "c073b88d-2774-4420-d233-ae5c9e57ccaf"
   },
   "outputs": [],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oeYvouItSSkl",
    "outputId": "dfb429ee-77b4-4867-836a-d29aa1e8b1eb"
   },
   "outputs": [],
   "source": [
    "# to check if everything loaded fine\n",
    "train_metadata.storm_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAyBcuGfSSwx"
   },
   "outputs": [],
   "source": [
    "full_metadata = train_metadata.merge(train_labels, on=\"image_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8tuhR65SS-E"
   },
   "outputs": [],
   "source": [
    "full_metadata['file_name'] = full_metadata.image_id.apply(lambda x: f'{directory}train/{x}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "y1ZJBOEBVWif",
    "outputId": "248a9016-f782-48c4-f8cd-7716ac7532e0"
   },
   "outputs": [],
   "source": [
    "full_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "H8oO6r1oVySg",
    "outputId": "51034487-12f2-4169-9ab4-93530c3fb463"
   },
   "outputs": [],
   "source": [
    "# load a single image and see how it is:\n",
    "img = mpimg.imread(f'{directory}sample_img_abs_010.jpg')\n",
    "print('image shape=',img.shape)\n",
    "plt.figure(figsize=[4.5,4])\n",
    "plt.pcolor(img)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAZMc_U5STK6"
   },
   "outputs": [],
   "source": [
    "def resize_crop_img(img):\n",
    "    # The function takes an array of a 366x366 image and converts it to a 64x64 shape.\n",
    "    # The input array for this exercise should be 366x366.\n",
    "    #----------------------------------------------\n",
    "    new_img_size = 256\n",
    "    # 1. Cropping a 256x256 size array that has the highest values (i.e the eye of cyclone and its surrounding)\n",
    "    n_passes = img.shape[0] - new_img_size\n",
    "    slice_avg = np.array([[img[row:row+new_img_size,col:col+new_img_size].mean() \\\n",
    "                           for col in range(n_passes)] for row in range(n_passes)])\n",
    "    max_index = np.unravel_index(slice_avg.argmax(), slice_avg.shape)\n",
    "    img_crop = img[max_index[0]:max_index[0]+new_img_size, max_index[1]:max_index[1]+new_img_size]\n",
    "    # 2. Reshaping the array from 256x256 to 64x64 (same as using a MaxPool2D with size=(2,2) and stride=2)\n",
    "    img_crop_resize = img_resize = img_crop.reshape(64, 4, 64, 4).max(-1).max(1)\n",
    "\n",
    "    # another way to do the above but using tensorflow. This is much slower. \n",
    "    # One can also consider using opencv for this part, as that library is very efficient for image processing.\n",
    "    \"\"\"\n",
    "    average_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(new_img_size, new_img_size), \n",
    "                                                       strides=(1, 1))\n",
    "    max_pool_2d = tf.keras.layers.MaxPool2D(pool_size=(4, 4), \n",
    "                                                       strides=(1, 1))\n",
    "    x = tf.constant(np.trunc(img))\n",
    "    slice_avg = average_pool_2d(tf.reshape(x, [1, 366, 366, 1]))\n",
    "    slice_avg = np.array(slice_avg[0,:,:,0]).reshape(111,111)\n",
    "    img_crop = x[max_index[1]:max_index[1]+new_img_size, max_index[2]:max_index[2]+new_img_size]\n",
    "    img_crop_resize = max_pool_2d(tf.reshape(x, [1, 256, 256, 1]))\n",
    "    \"\"\"\n",
    "\n",
    "    return img_crop_resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "E7ZU8pNYpBxO",
    "outputId": "7fc27d3a-c4d4-47da-a8f3-bc427d7f692c"
   },
   "outputs": [],
   "source": [
    "# load a single image and see how the resizing works:\n",
    "img = mpimg.imread(f'{directory}/sample_img_abs_010.jpg')\n",
    "print('Original image shape=',img.shape)\n",
    "plt.figure(figsize=[4.5,4])\n",
    "plt.pcolor(img, cmap='RdYlBu')\n",
    "plt.colorbar()\n",
    "plt.figure(figsize=[4.5,4])\n",
    "plt.pcolor(resize_crop_img(img), cmap='RdYlBu')\n",
    "print('Resized image shape=',resize_crop_img(img).shape)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_Xk7BA8Svc_"
   },
   "outputs": [],
   "source": [
    "# Add a temporary column for number of images per storm\n",
    "images_per_storm = full_metadata.groupby(\"storm_id\").size().to_frame(\"images_per_storm\")\n",
    "full_metadata = full_metadata.merge(images_per_storm, how=\"left\", on=\"storm_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmqnFXyQSvZP"
   },
   "outputs": [],
   "source": [
    "# Each storm is sorted by relative time\n",
    "# Identify the final 20% of images per storm\n",
    "full_metadata[\"pct_of_storm\"] = (\n",
    "    full_metadata.groupby(\"storm_id\").cumcount() / full_metadata.images_per_storm\n",
    ")\n",
    "train = full_metadata[full_metadata.pct_of_storm < 0.8].drop(\n",
    "    [\"images_per_storm\", \"pct_of_storm\"], axis=1\n",
    ")\n",
    "val = full_metadata[full_metadata.pct_of_storm >= 0.8].drop(\n",
    "    [\"images_per_storm\", \"pct_of_storm\"], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "2O8nWUhBSvVt",
    "outputId": "2b33eb16-ab92-4344-c4b2-3e7950bbea5a"
   },
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzj675_oSvPq"
   },
   "source": [
    "# 3. Resizing training data (only done if limited memory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ws2gI1Gaqyie"
   },
   "outputs": [],
   "source": [
    "# This part is only done to reduce input file size and to be able to run the code using limited memory \n",
    "# on Google Colab. If you are not limited with computation power, you can skip sections 3 & 4.\n",
    "# The resulting files from section 3 and 4 are already saved in the shared Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5AKBhlOSvCT"
   },
   "outputs": [],
   "source": [
    "def extract_data_chunk(i, period):\n",
    "    chunk_size = 1000\n",
    "    if period == 'train':\n",
    "        if i == len(train)//chunk_size:\n",
    "            x_train = np.stack(([resize_crop_img(mpimg.imread(train.file_name.iloc[x])/255) \\\n",
    "                                 for x in range(chunk_size*i,len(train))]),\n",
    "                               axis=2).transpose(2, 0, 1).reshape(chunk_size*i-len(train),64,64,1)\n",
    "        else:\n",
    "            x_train = np.stack(([resize_crop_img(mpimg.imread(train.file_name.iloc[x])/255) \\\n",
    "                                for x in range(chunk_size*i,chunk_size*(i+1))]),\n",
    "                               axis=2).transpose(2, 0, 1).reshape(chunk_size,64,64,1)\n",
    "        np.save(f'{directory}/chunks/x_train_part_{i:03.0f}.npy',\n",
    "                x_train, allow_pickle=False)\n",
    "\n",
    "    elif period == 'val':\n",
    "        if i == len(val)//chunk_size:\n",
    "            x_val = np.stack(([resize_crop_img(mpimg.imread(val.file_name.iloc[x])/255) \\\n",
    "                               for x in range(chunk_size*i,len(val))]),\n",
    "                             axis=2).transpose(2, 0, 1).reshape(chunk_size*i-len(val),64,64,1)\n",
    "        else:\n",
    "            x_val = np.stack(([resize_crop_img(mpimg.imread(val.file_name.iloc[x])/255) \\\n",
    "                                for x in range(chunk_size*i,chunk_size*(i+1))]),\n",
    "                             axis=2).transpose(2, 0, 1).reshape(chunk_size,64,64,1)\n",
    "        np.save(f'{directory}/chunks/x_val_part_{i:03.0f}.npy',\n",
    "                x_val, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eI2dYOZdSTWt"
   },
   "outputs": [],
   "source": [
    "for i in range(14):\n",
    "    extract_data_chunk(i, period='val')\n",
    "for i in range(57):\n",
    "    extract_data_chunk(i, period='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGHNPQ6VTf5s"
   },
   "outputs": [],
   "source": [
    "# Stacking images\n",
    "dir_str = f'{directory}chunks'\n",
    "x_train = np.concatenate(([np.load(f'{dir_str}/x_train_part_{i:03.0f}.npy') for i in range(57)]),\n",
    "                    axis=0)\n",
    "x_val = np.concatenate(([np.load(f'{dir_str}/x_val_part_{i:03.0f}.npy') for i in range(14)]),\n",
    "                    axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FScXFfdYTf1V"
   },
   "outputs": [],
   "source": [
    "x_train.shape, x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hl6SRvPTfwh"
   },
   "outputs": [],
   "source": [
    "len(train), len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-_eO6RITfrx"
   },
   "outputs": [],
   "source": [
    "y_train = train.wind_speed.iloc[:len(x_train)].values\n",
    "y_val = val.wind_speed.iloc[:len(x_val)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f--uWtDUTfnJ"
   },
   "outputs": [],
   "source": [
    "y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofOTKDL1Tfi1"
   },
   "outputs": [],
   "source": [
    "#np.save(f'{directory}x_train.npy', x_train, allow_pickle=False)\n",
    "#np.save(f'{directory}x_val.npy', x_val, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9n6EPH8TfeQ"
   },
   "source": [
    "# 4. Reshaping test data (only done if limited memory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaR7dz_jTfYK"
   },
   "outputs": [],
   "source": [
    "test_metadata  = pd.read_csv(f'{directory}test_set_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VD0GlBfJTfOc"
   },
   "outputs": [],
   "source": [
    "test_metadata['file_name'] = test_metadata.image_id.apply(lambda x: f'{directory}/test/{x}.jpg')\n",
    "test_metadata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RU2F8-bUFCU"
   },
   "outputs": [],
   "source": [
    "len(test_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WDr8oaVWUE8y"
   },
   "outputs": [],
   "source": [
    "def extract_testdata_chunk(i, test_metadata):\n",
    "    chunk_size = 1000\n",
    "    if i == len(test_metadata)//chunk_size:\n",
    "        x_test = np.stack(([resize_crop_img(mpimg.imread(test_metadata.file_name.iloc[x])/255) \\\n",
    "                              for x in range(chunk_size*i,len(test_metadata))]),\n",
    "                            axis=2).transpose(2, 0, 1).reshape(chunk_size*i-len(test_metadata),64,64,1)\n",
    "    else:\n",
    "        x_test = np.stack(([resize_crop_img(mpimg.imread(test_metadata.file_name.iloc[x])/255) \\\n",
    "                            for x in range(chunk_size*i,chunk_size*(i+1))]),\n",
    "                            axis=2).transpose(2, 0, 1).reshape(chunk_size,64,64,1)\n",
    "    np.save(f'{directory}/chunks/x_test_part_{i:03.0f}.npy',\n",
    "            x_test, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZD3No8yiUE4G"
   },
   "outputs": [],
   "source": [
    "for i in range(45):\n",
    "    start_time = time.time()\n",
    "    extract_testdata_chunk(i, test_metadata)\n",
    "    print(f'Part {i} is done!')\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWR_W2y6UEy9"
   },
   "outputs": [],
   "source": [
    "# Stacking images\n",
    "dir_str = f'{directory}/chunks'\n",
    "x_test = np.concatenate(([np.load(f'{dir_str}/x_test_part_{i:03.0f}.npy') for i in range(45)]),\n",
    "                    axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxyKbkXNUEuC"
   },
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTuKQyeoUEoP"
   },
   "outputs": [],
   "source": [
    "#np.save(f'{directory}x_test.npy', x_test, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEbQAcaeUEjo"
   },
   "source": [
    "# 5. Rotating and duplicating low frequency images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G8HajsJHEyM"
   },
   "source": [
    "'''\n",
    "To load the data from the shared Google drive, open the shared drive, right click on the folder, and select \"Add shortcut to Drive\" and select My Drive. You will then be able to access and load the data in Google Colab.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4lYzxYkUEep"
   },
   "outputs": [],
   "source": [
    "x_train = np.load(f'{directory}x_train.npy')\n",
    "x_val = np.load(f'{directory}x_val.npy')\n",
    "y_train = train.wind_speed.iloc[:len(x_train)].values\n",
    "y_val = val.wind_speed.iloc[:len(x_val)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WK8_INGhUEZ2",
    "outputId": "44fd7214-4346-444f-a167-34e62d2cd531"
   },
   "outputs": [],
   "source": [
    "x_train.shape, x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4v7dQCXUEVI",
    "outputId": "58ca998e-c9ee-4e9f-b1f3-0974263a2fa1"
   },
   "outputs": [],
   "source": [
    "y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSRCxVSnUEQB"
   },
   "outputs": [],
   "source": [
    "# For data augmentation and rotating input images, I am only focusing on the images corresponding to wind speeds\n",
    "# above 40. The value 40 is an arbitrary value to make sure that the code is still able to be run on Google Colab.\n",
    "train_highwind = train.reset_index(drop=True)[train.reset_index(drop=True).wind_speed>40]\n",
    "#train_highwind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrWATETcUEK-"
   },
   "outputs": [],
   "source": [
    "x_highwind = x_train[train_highwind.index,:,:,:]\n",
    "y_highwind = y_train[train_highwind.index]\n",
    "x_highwind_rot1 = np.rot90(x_highwind, axes=(1,2))\n",
    "x_highwind_rot2 = np.rot90(x_highwind, 2, axes=(1,2))\n",
    "x_highwind_rot3 = np.rot90(x_highwind, 3, axes=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "8d66RTJ3UEGZ",
    "outputId": "3c042f8d-9dd2-4dc0-f4ee-4bb38016ce1e"
   },
   "outputs": [],
   "source": [
    "# Sample plot for rotating: \n",
    "plt.figure(figsize=[15,3.5])\n",
    "plt.subplot(141)\n",
    "plt.pcolor(x_highwind[0,:,:,0], cmap='RdYlBu')\n",
    "plt.subplot(142)\n",
    "plt.pcolor(x_highwind_rot1[0,:,:,0],cmap='RdYlBu')\n",
    "plt.subplot(143)\n",
    "plt.pcolor(x_highwind_rot2[0,:,:,0],cmap='RdYlBu')\n",
    "plt.subplot(144)\n",
    "plt.pcolor(x_highwind_rot3[0,:,:,0],cmap='RdYlBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZ2-QHh0V68y"
   },
   "outputs": [],
   "source": [
    "x_train_extra = np.concatenate((x_train,x_highwind_rot1,x_highwind_rot2,x_highwind_rot3), axis=0)\n",
    "y_train_extra = np.concatenate((y_train,y_highwind,y_highwind,y_highwind), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncY8idi4V63a",
    "outputId": "83047792-3e53-4da9-97e3-f779db0acb7e"
   },
   "outputs": [],
   "source": [
    "x_train.shape, x_train_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKCtbCcIV6yr",
    "outputId": "b2727afb-399c-4291-9d67-84d3b57d6b77"
   },
   "outputs": [],
   "source": [
    "y_train.shape, y_train_extra.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KzEHdYjWGEy"
   },
   "source": [
    "# 6. Developing the CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UHpbKSrV6tY"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import AveragePooling2D, BatchNormalization, Conv2D, Dense, Dropout, Flatten, MaxPool2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping#, ModelCheckpoint\n",
    "#from tensorflow.keras.optimizers import SGD\n",
    "from functools import partial\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAWkzxpUV6on"
   },
   "outputs": [],
   "source": [
    "alpha = 0.0001\n",
    "padding = 'same' #'valid'\n",
    "activation = tf.keras.layers.LeakyReLU(alpha=alpha) #'relu'\n",
    "optimizer = 'adam' #SGD(learning_rate=0.01)\n",
    "\n",
    "model = Sequential()\n",
    "#first layer\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=(64, 64, 1), \n",
    "                activation=activation, padding=padding))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), padding='valid'))\n",
    "model.add(Dropout(0.35))\n",
    "# second layer\n",
    "model.add(Conv2D(filters=128, kernel_size=(2,2), activation=activation))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), padding='valid'))\n",
    "model.add(Dropout(0.3))\n",
    "# third layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(2,2), activation=activation))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), padding=padding))\n",
    "model.add(Dropout(0.3))\n",
    "# fourth layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(2,2), activation=activation))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), padding=padding))\n",
    "model.add(Dropout(0.3))\n",
    "# fifth layer\n",
    "model.add(Conv2D(filters=512, kernel_size=(2,2), activation=activation))\n",
    "model.add(Dropout(0.3))\n",
    "# sixth layer\n",
    "model.add(Conv2D(filters=512, kernel_size=(2,2), activation=activation))\n",
    "model.add(Dropout(0.3))\n",
    "# Dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation=activation))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1024, activation=activation))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation=activation))\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "#model.add(MaxPool2D(pool_size=(2, 2), padding='valid'))\n",
    "#model.add(AveragePooling2D(pool_size=(2, 2), padding='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0Am0SJAV6jj",
    "outputId": "5ef5d4a7-c9d4-4a4e-e4f1-b8a606a085cd"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blUXtauQV6eY"
   },
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=8, verbose=0, mode='min')\n",
    "#mcp_save = ModelCheckpoint('weights.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "#reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKziJ5WyV6Zx"
   },
   "outputs": [],
   "source": [
    "del x_train #to make the memory available for training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WY7fMBFnV6Ur"
   },
   "source": [
    "# 7. Model fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RShoaB2fV6P6",
    "outputId": "b6d5866d-4822-4d33-8e8d-d7537a245216"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train_extra,y_train_extra,epochs=30,validation_data=(x_val,y_val), batch_size=800)\n",
    "#model.fit(x_train_extra,y_train_extra,epochs=15,validation_data=(x_val,y_val), batch_size=1000,callbacks=[earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "VSzkKpVUTeh9",
    "outputId": "ac4ca1ef-714c-4dc2-c110-255e4a800b89"
   },
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model \n",
    "model.save('tc_model') # the input to model.save() should a path that you would like to save the model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vdMusAm0LO2p",
    "outputId": "5a34f8ae-2bda-4fdb-df2f-4515a27d9837"
   },
   "outputs": [],
   "source": [
    "# Loading the pretrained model (if already available):\n",
    "model = load_model(f'{directory}saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etacPyU2XVEM"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "MoUhg7eOXVZa",
    "outputId": "789daae5-403e-4797-efe0-0f91d33ef5cb"
   },
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(y_val.reshape(-1,1), columns=['truth'])\n",
    "preds['model'] = predictions\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSy0mPmFXXMt"
   },
   "outputs": [],
   "source": [
    "preds['diff'] = abs(preds['truth']-preds['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18unOrytXXGV"
   },
   "outputs": [],
   "source": [
    "val['prediction'] = preds.model.values.astype(int)\n",
    "val = val.drop(columns=['file_name'])\n",
    "# val.to_csv('prediction_val_Radiant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQTaM4poXXAG",
    "outputId": "9cba0e53-3847-418c-98f0-b712eda8e84d"
   },
   "outputs": [],
   "source": [
    "round(mean_squared_error(preds.model, preds.truth, squared=False),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYERXhSDXW5c",
    "outputId": "96a69664-cfc2-4c6a-a49f-d792364a398f"
   },
   "outputs": [],
   "source": [
    "round(mean_absolute_error(preds.truth,preds.model),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "gSKRAu11XWM3",
    "outputId": "c8edce25-98c2-48ca-a5a6-2456e871aac1"
   },
   "outputs": [],
   "source": [
    "preds.groupby('truth').mean()['diff'].rolling(window=10).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrlIHy0HX4Fq"
   },
   "source": [
    "# 8. Prediction for test period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4_zjmUaX3-n"
   },
   "outputs": [],
   "source": [
    "# we need to delete the training data and release memory to be able to run the model for test period.\n",
    "del x_train_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RmOGB8nX320"
   },
   "outputs": [],
   "source": [
    "x_test = np.load(f'{directory}/x_test.npy')\n",
    "test_metadata  = pd.read_csv(f'{directory}/test_set_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkSDgwSJX3vT",
    "outputId": "68334479-fa91-4333-e17a-930a7fa9c6b8"
   },
   "outputs": [],
   "source": [
    "test_pred = model.predict(x_test)\n",
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEwPgGyTX3pA"
   },
   "outputs": [],
   "source": [
    "test_metadata['prediction'] = test_pred[:,0]\n",
    "test_metadata.to_csv('prediction_test_Radiant.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KqjnD3hX3iV"
   },
   "outputs": [],
   "source": [
    "test_metadata['wind_speed'] = test_metadata.prediction.apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTqloU5dX3bj"
   },
   "outputs": [],
   "source": [
    "#saving submission\n",
    "test_metadata[['image_id','wind_speed']].set_index('image_id').to_csv('Submission_Radiant.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "S2yYKmo_44Vv",
    "kzj675_oSvPq",
    "D9n6EPH8TfeQ"
   ],
   "name": "CNN_Hurricane_RadiantEarth.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
